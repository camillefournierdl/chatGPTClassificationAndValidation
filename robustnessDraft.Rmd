---
title: "robustnessDraft"
output: html_document
---

```{r identfying papers that are not in social science journals}

library(tidyverse)
library(stringi)
library(fuzzyjoin)

### !!!!! ihi something weird about papers that have a translation

litReviewSub <- read.csv("dataNew/combined_lit-socialscience.csv")

clean_title <- function(x) {
  x %>%
    stri_trans_general("Latin-ASCII") %>%  # turn “é” → “e”
    tolower() %>%                          # lowercase
    gsub("[^[:alnum:] ]+", "", .) %>%      # drop punctuation
    trimws()                               # trim whitespace
}

litReviewSub <- litReviewSub %>%
  select(Title) %>% 
  mutate(title_clean = clean_title(Title))

litReviewSub <- litReviewSub %>%
  group_by(title_clean) %>% 
  summarize(title_clean = first(title_clean))

datasetID <- read.csv("dataNew/datasetToClassifyGPT.csv")

datasetID <- datasetID %>%
  mutate(title_clean = clean_title(Title))

litReviewMerged2 <- stringdist_left_join(
  litReviewSub, datasetID,
  by = "title_clean",
  method = "lv",        # Levenshtein
  max_dist = 15,         # tweak this
  distance_col = "dist" # keep the distance
) %>%
  arrange(-dist) %>%          # largest distances first
  group_by(ID) %>% 
  summarize(ID = first(ID),
            Title = first(Title))

write.csv(litReviewMerged2, "output/robustnessSocialScience.csv", row.names = F)

```

# ```{r wasting time matching something that should not be compliated}
# 
# datasetClassification <- read.csv("output/dataClassifToPlot.csv")
# 
# test <- read.csv("dataNew/allgpt-lit_w_countries.csv") %>% 
#   select(X , Title) %>% 
#   rename(ID = X)
# 
# datasetClassificationMerged <- merge(datasetClassification, test, by = "ID")
# 
# write.csv(datasetClassificationMerged, "output/dataClassifToPlotTemporaryRobust.csv", row.names = F)
# 
# ```

```{r}
library(tidyverse)
"%ni%" = Negate("%in%")

dataToSubset <- read.csv("output/robustnessSocialScience.csv")

datasetClassification <- read.csv("output/dataClassifToPloto4mini.csv")

datasetCountries <- read.csv("output/fullClassifCountryo4mini.csv")

datasetCountries <- datasetCountries %>% 
  select(ID, countryPartialF) %>% 
  rename(country = countryPartialF)

datasetClassificationCountries <- merge(datasetClassification, datasetCountries,
                                        by = "ID") %>% 
  select(-reader)

datasetClassificationCountries$country <- ifelse(datasetClassificationCountries$country == "None", "Unknown", datasetClassificationCountries$country)

# here is the step where we only keep papers in social science journals
datasetClassificationCountries <- subset(datasetClassificationCountries, ID %in% unique(dataToSubset$ID))

table(datasetClassificationCountries$country)



# 1) split into one row per country
datasetClassificationCountries_long <- datasetClassificationCountries %>%
  separate_rows(country, sep = ",\\s*") %>%
  mutate(countries = str_trim(country))

datasetClassificationCountries_long$countries <- ifelse(datasetClassificationCountries_long$countries == "Turkyie", "Turkey",
                                                        ifelse(datasetClassificationCountries_long$countries %in% c("Wales", "England"), "United Kingdom",
                                                               datasetClassificationCountries_long$countries))
# fix country names 
library(countrycode)

# 1. Direct mapping
datasetClassificationCountries_long_clean <- datasetClassificationCountries_long %>%
  mutate(
    iso3 = countrycode(countries,
                       origin      = "country.name",
                       destination = "iso3c")
  ) %>% 
  filter(countries %ni% c("", " ", "Kosovo", "Southeast Asia"))

### add income groups to the dataset

incomeGroups <- read.csv("dataNew/worldBankIncomeGroups.csv", sep = ";")
incomeGroups$Income.group <- ifelse(incomeGroups$Code == "VEN", "Upper middle income", incomeGroups$Income.group)

datasetClassificationCountries_long_clean <- merge(datasetClassificationCountries_long_clean, incomeGroups, by.x = "iso3", by.y = "Code", all.x = T, all.y = F)

# 2) get total papers by country
papers_by_country <- datasetClassificationCountries_long_clean %>%
  group_by(country, iso3, .drop = F) %>% 
  count(iso3, name = "n_papers") %>%
  arrange(desc(n_papers)) %>% 
  merge(., incomeGroups, by.x = "iso3", by.y = "Code", all.x = T, all.y = F)

papers_by_country$Income.group <- factor(
  papers_by_country$Income.group,
  levels = c(
    "High income",
    "Upper middle income",
    "Lower middle income",
    "Low income"
  ),
  ordered = TRUE
)

pCountryPaperIncome <- papers_by_country %>%
  slice_max(n_papers, n = 20) %>%                     # top 30
  mutate(
    # 1) reorder by paper count (so bars go largest→smallest),
    country = fct_reorder(country, n_papers)
  ) %>%
  ggplot(aes(x = country, y = n_papers, fill = Income.group)) +
    geom_col() +
    coord_flip() +
    scale_fill_discrete(na.value = 'grey')+
    labs(
      x = "Country",
      y = "Number of papers",
      title = "Top 20 Countries by Paper Count"
    ) +
    theme_minimal()

ggsave(plot = pCountryPaperIncome, filename = paste("robustness/plots/countryPapersIncome.png", sep = ""),
       dpi=600, width = 16, height = 12, units='cm')


#### make a map ####

### getting the data together
library(rnaturalearth)
library(terra)
library(tidyterra)

world_countries <- vect(ne_countries(scale = "large", returnclass = "sf"))

world_countries$iso_a3 <- ifelse(world_countries$sovereignt == "Somaliland", "SOM", world_countries$iso_a3)

world_countries_merged <- merge(world_countries, papers_by_country, by.x = "adm0_a3", by.y = "iso3", all.x = T)

world_countries_merged$n_papers <- ifelse(is.na(world_countries_merged$n_papers), 0, world_countries_merged$n_papers)

my_breaks = c(1, 5, 20, 50, 150)

map_plot_reference <- ggplot() +
  geom_spatvector(data = world_countries_merged, aes(fill = n_papers)) +
  scale_fill_gradient(name = "nb of papers", trans = "log",
                        breaks = my_breaks, labels = my_breaks)+
  theme_void()

plot(map_plot_reference)

library(scales)          # for label formatting

# 1) Filter out Antarctica from your data:
world_no_antarctica <- 
  world_countries_merged %>%
  filter(name != "Antarctica")

# 2) Build the map with integer labels and no decimals:
map_plot_viridis_clean <- ggplot() +
  geom_spatvector(
    data = world_no_antarctica,
    aes(fill = n_papers),
    color = "grey70",
    size  = 0.1
  ) +
  scale_fill_viridis_c(
    name    = "Papers",
    # option  = "plasma",
    trans   = "log10",
    breaks  = my_breaks,
    labels  = label_number(accuracy = 1,    # round to whole numbers
                           big.mark = ",",
                           trim     = TRUE),
    na.value = "grey80"
  ) +
  coord_sf(expand = FALSE) +
  labs(
    title    = "Global Publication Counts by Country",
    # subtitle = "Log₁₀ scale, viridis “plasma” palette"
  ) +
  theme_void(base_size = 14) +
  theme(
    legend.position  = "bottom",
    legend.key.width = unit(2, "cm"),
    plot.title       = element_text(face = "bold", hjust = 0.5),
    plot.subtitle    = element_text(hjust = 0.5)
  )

# Print it
map_plot_viridis_clean

ggsave(plot = map_plot_viridis_clean, filename = paste("robustness/plots/mapPapersCount.png", sep = ""),
       dpi=600, width = 16, height = 12, units='cm')

##### categories over time #####
# 

# data with ID year 
datasetYear <- read.csv("dataNew/datasetToClassifyGPT.csv") %>% 
  select(ID, Year)

datasetClassificationCountries_long_clean <- merge(datasetClassificationCountries_long_clean,
                                                   datasetYear,
                                                   by = "ID", all.x = T, all.y = F)

# 2) count papers per country-year
yearly <- datasetClassificationCountries_long_clean %>%
  count(Income.group, Year, name = "n_year") 

# 3) ensure every country has an entry for every year in your span
years <- seq(min(datasetClassificationCountries_long_clean$Year),
             max(datasetClassificationCountries_long_clean$Year))

yearly_complete <- yearly %>%
  complete(Income.group, Year = years, fill = list(n_year = 0))

# 4) compute cumulative sum within each country
cum <- yearly_complete %>%
  group_by(Income.group) %>%
  arrange(Year, .by_group = TRUE) %>%
  mutate(n_cumulative = cumsum(n_year)) %>%
  ungroup()

cum$Income.group <- ifelse(is.na(cum$Income.group), "Unknown", cum$Income.group)

cum$Income.group <- factor(
  cum$Income.group,
  levels = c(
    "High income",
    "Upper middle income",
    "Lower middle income",
    "Low income"
    # "Unknown"
  ),
  ordered = TRUE
)

ppapersIncome <- subset(cum, !is.na(Income.group)) %>%
  ggplot(aes(x = Year, y = n_cumulative, col = Income.group)) +
    geom_line(linewidth = 1) +
    labs(
      x = "Year",
      y = "Cumulative # of Papers",
      title = "Growth of Papers by income group"
    ) +
    theme_minimal()

ggsave(plot = ppapersIncome, filename = paste("robustness/plots/incomePaperYear.png", sep = ""),
       dpi=600, width = 16, height = 8, units='cm')

# then cumulative for different categories of papers

# 1) pivot your dummy columns into long form
df_cat_long <- datasetClassificationCountries_long_clean %>%
  pivot_longer(
    cols = c(Perceptions, Priority, Policy, Health, Behavior),
    names_to  = "category",
    values_to = "flag"
  ) %>%
  # keep only “paper belongs to this category”
  filter(flag == 1) %>%
  select(-flag)

# 2) count per year × income_group × category
yearly_cat <- df_cat_long %>%
  count(Year, Income.group, category, name = "n_papers")

# 3) fill zeros so lines don’t break
all_years <- seq(min(datasetClassificationCountries_long_clean$Year), max(datasetClassificationCountries_long_clean$Year))

yearly_cat <- yearly_cat %>%
  complete(
    Year, Income.group, category,
    fill = list(n_papers = 0)
  )

yearly_cat$Income.group <- factor(
  yearly_cat$Income.group,
  levels = c(
    "High income",
    "Upper middle income",
    "Lower middle income",
    "Low income"
    # "Unknown"
  ),
  ordered = TRUE
)

yearly_cat <- yearly_cat %>%
  group_by(Income.group, category) %>%
  arrange(Year, .by_group = TRUE) %>%
  mutate(n_cum = cumsum(n_papers)) %>%
  ungroup() %>% 
  # reorder so levels go from lowest to highest n_cum at the final Year
  mutate(category = fct_reorder2(category, Year, n_cum))

# 4) plot: one facet per category, colored by income group
ppapersIncomeYear <- ggplot(subset(yearly_cat, !is.na(Income.group)),
       aes(x = Year, y = n_cum, color = category)) +
  geom_line(linewidth = 1) +
  facet_wrap(~ Income.group, scales = "free_y") +
  theme_minimal()

ggsave(plot = ppapersIncomeYear, filename = paste("robustness/plots/incomeCategoryPapersYear.png", sep = ""),
       dpi=600, width = 20, height = 12, units='cm')

# a) sum across incomes, then compute cumulative
yearly_cat_agg <- yearly_cat %>%
  group_by(Year, category) %>%
  summarise(n_year = sum(n_papers), .groups="drop") %>%
  arrange(category, Year) %>%
  group_by(category) %>%
  mutate(n_cum = cumsum(n_year)) %>%
  ungroup()

# b) plot one line per category
ppapersYear <- ggplot(yearly_cat_agg,
       aes(x = Year, y = n_cum, color = category)) +
  geom_line(linewidth = 1) +
  theme_minimal() +
  labs(
    x     = "Year",
    y     = "Cumulative # of Papers",
    color = "Category",
    title = "Cumulative Papers by Category (all incomes combined)"
  )

ggsave(plot = ppapersYear, filename = paste("robustness/plots/countryPapersYear.png", sep = ""),
       dpi=600, width = 16, height = 8, units='cm')


```

